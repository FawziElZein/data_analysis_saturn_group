{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: 403 Client Error: Forbidden for url: https://www.coingecko.com/price_charts/export/1/usd.csv\n"
     ]
    }
   ],
   "source": [
    "from pandas_data_handler import download_csv_to_dataframe\n",
    "from lookups import CsvUrl\n",
    "\n",
    "# Example usage:\n",
    "data_frame = download_csv_to_dataframe(CsvUrl.BITCOIN_USD_HISTORICAL_DATA)\n",
    "\n",
    "if data_frame is not None:\n",
    "    print(data_frame.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Tweet Id</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/01/2021 00:58</td>\n",
       "      <td>1.344810e+18</td>\n",
       "      <td>@PPathole Dojo isnât needed, but will make s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02/01/2021 03:20</td>\n",
       "      <td>1.345210e+18</td>\n",
       "      <td>@comma_ai Tesla Full Self-Driving will work at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>02/01/2021 12:23</td>\n",
       "      <td>1.345340e+18</td>\n",
       "      <td>@newscientist Um, we have giant fusion reactor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02/01/2021 14:51</td>\n",
       "      <td>1.345380e+18</td>\n",
       "      <td>So proud of the Tesla team for achieving this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>02/01/2021 14:59</td>\n",
       "      <td>1.345380e+18</td>\n",
       "      <td>@flcnhvy Tesla is responsible for 2/3 of all t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2988</th>\n",
       "      <td>30/12/2021 18:23</td>\n",
       "      <td>1.476620e+18</td>\n",
       "      <td>@BLKMDL3 @mims Predicting macroeconomics is ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2989</th>\n",
       "      <td>30/12/2021 20:28</td>\n",
       "      <td>1.476650e+18</td>\n",
       "      <td>@CSmithson80 @heydave7 @BLKMDL3 @mims This cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2990</th>\n",
       "      <td>30/12/2021 20:47</td>\n",
       "      <td>1.476660e+18</td>\n",
       "      <td>@tesla_raj Many UI improvements coming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2991</th>\n",
       "      <td>30/12/2021 21:11</td>\n",
       "      <td>1.476660e+18</td>\n",
       "      <td>@roshanpateI ð¤£ $7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2992</th>\n",
       "      <td>31/12/2021 02:23</td>\n",
       "      <td>1.476740e+18</td>\n",
       "      <td>@TheBabylonBee ð¤£</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2993 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Datetime      Tweet Id  \\\n",
       "0     01/01/2021 00:58  1.344810e+18   \n",
       "1     02/01/2021 03:20  1.345210e+18   \n",
       "2     02/01/2021 12:23  1.345340e+18   \n",
       "3     02/01/2021 14:51  1.345380e+18   \n",
       "4     02/01/2021 14:59  1.345380e+18   \n",
       "...                ...           ...   \n",
       "2988  30/12/2021 18:23  1.476620e+18   \n",
       "2989  30/12/2021 20:28  1.476650e+18   \n",
       "2990  30/12/2021 20:47  1.476660e+18   \n",
       "2991  30/12/2021 21:11  1.476660e+18   \n",
       "2992  31/12/2021 02:23  1.476740e+18   \n",
       "\n",
       "                                                   Text  \n",
       "0     @PPathole Dojo isnât needed, but will make s...  \n",
       "1     @comma_ai Tesla Full Self-Driving will work at...  \n",
       "2     @newscientist Um, we have giant fusion reactor...  \n",
       "3     So proud of the Tesla team for achieving this ...  \n",
       "4     @flcnhvy Tesla is responsible for 2/3 of all t...  \n",
       "...                                                 ...  \n",
       "2988  @BLKMDL3 @mims Predicting macroeconomics is ch...  \n",
       "2989  @CSmithson80 @heydave7 @BLKMDL3 @mims This cha...  \n",
       "2990             @tesla_raj Many UI improvements coming  \n",
       "2991                               @roshanpateI ð¤£ $7  \n",
       "2992                                @TheBabylonBee ð¤£  \n",
       "\n",
       "[2993 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 1, 'name': 'Bitcoin', 'symbol': 'BTC', 'slug': 'bitcoin', 'num_market_pairs': 10475, 'date_added': '2010-07-13T00:00:00.000Z', 'tags': ['mineable', 'pow', 'sha-256', 'store-of-value', 'state-channel', 'coinbase-ventures-portfolio', 'three-arrows-capital-portfolio', 'polychain-capital-portfolio', 'binance-labs-portfolio', 'blockchain-capital-portfolio', 'boostvc-portfolio', 'cms-holdings-portfolio', 'dcg-portfolio', 'dragonfly-capital-portfolio', 'electric-capital-portfolio', 'fabric-ventures-portfolio', 'framework-ventures-portfolio', 'galaxy-digital-portfolio', 'huobi-capital-portfolio', 'alameda-research-portfolio', 'a16z-portfolio', '1confirmation-portfolio', 'winklevoss-capital-portfolio', 'usv-portfolio', 'placeholder-ventures-portfolio', 'pantera-capital-portfolio', 'multicoin-capital-portfolio', 'paradigm-portfolio', 'bitcoin-ecosystem', 'ftx-bankruptcy-estate'], 'max_supply': 21000000, 'circulating_supply': 19491568, 'total_supply': 19491568, 'infinite_supply': False, 'platform': None, 'cmc_rank': 1, 'self_reported_circulating_supply': None, 'self_reported_market_cap': None, 'tvl_ratio': None, 'last_updated': '2023-09-21T04:05:00.000Z', 'quote': {'USD': {'price': 27021.62556040232, 'volume_24h': 13056355033.884113, 'volume_change_24h': -7.5557, 'percent_change_1h': 0.1316774, 'percent_change_24h': -0.32990617, 'percent_change_7d': 2.84616858, 'percent_change_30d': 3.80199017, 'percent_change_60d': -9.51466949, 'percent_change_90d': -10.00606592, 'market_cap': 526693852081.11993, 'market_cap_dominance': 49.176, 'fully_diluted_market_cap': 567454136768.45, 'tvl': None, 'last_updated': '2023-09-21T04:05:00.000Z'}}}, {'id': 1027, 'name': 'Ethereum', 'symbol': 'ETH', 'slug': 'ethereum', 'num_market_pairs': 7415, 'date_added': '2015-08-07T00:00:00.000Z', 'tags': ['pos', 'smart-contracts', 'ethereum-ecosystem', 'coinbase-ventures-portfolio', 'three-arrows-capital-portfolio', 'polychain-capital-portfolio', 'binance-labs-portfolio', 'blockchain-capital-portfolio', 'boostvc-portfolio', 'cms-holdings-portfolio', 'dcg-portfolio', 'dragonfly-capital-portfolio', 'electric-capital-portfolio', 'fabric-ventures-portfolio', 'framework-ventures-portfolio', 'hashkey-capital-portfolio', 'kenetic-capital-portfolio', 'huobi-capital-portfolio', 'alameda-research-portfolio', 'a16z-portfolio', '1confirmation-portfolio', 'winklevoss-capital-portfolio', 'usv-portfolio', 'placeholder-ventures-portfolio', 'pantera-capital-portfolio', 'multicoin-capital-portfolio', 'paradigm-portfolio', 'injective-ecosystem', 'layer-1', 'ftx-bankruptcy-estate'], 'max_supply': None, 'circulating_supply': 120225540.83014292, 'total_supply': 120225540.83014292, 'infinite_supply': True, 'platform': None, 'cmc_rank': 2, 'self_reported_circulating_supply': None, 'self_reported_market_cap': None, 'tvl_ratio': None, 'last_updated': '2023-09-21T04:05:00.000Z', 'quote': {'USD': {'price': 1622.5451268449574, 'volume_24h': 5222706717.594132, 'volume_change_24h': 19.3504, 'percent_change_1h': 0.20007915, 'percent_change_24h': -0.87723409, 'percent_change_7d': 0.10631929, 'percent_change_30d': -2.44602702, 'percent_change_60d': -13.25928636, 'percent_change_90d': -13.81025803, 'market_cap': 195071365396.24783, 'market_cap_dominance': 18.2131, 'fully_diluted_market_cap': 195071365396.25, 'tvl': None, 'last_updated': '2023-09-21T04:05:00.000Z'}}}, {'id': 825, 'name': 'Tether USDt', 'symbol': 'USDT', 'slug': 'tether', 'num_market_pairs': 62580, 'date_added': '2015-02-25T00:00:00.000Z', 'tags': ['payments', 'stablecoin', 'asset-backed-stablecoin', 'avalanche-ecosystem', 'solana-ecosystem', 'arbitrum-ecosytem', 'moonriver-ecosystem', 'injective-ecosystem', 'bnb-chain', 'usd-stablecoin', 'optimism-ecosystem'], 'max_supply': None, 'circulating_supply': 83191016572.37025, 'total_supply': 86426198726.43625, 'platform': {'id': 1027, 'name': 'Ethereum', 'symbol': 'ETH', 'slug': 'ethereum', 'token_address': '0xdac17f958d2ee523a2206206994597c13d831ec7'}, 'infinite_supply': True, 'cmc_rank': 3, 'self_reported_circulating_supply': None, 'self_reported_market_cap': None, 'tvl_ratio': None, 'last_updated': '2023-09-21T04:05:00.000Z', 'quote': {'USD': {'price': 1.0001648323106935, 'volume_24h': 20878348354.632835, 'volume_change_24h': 3.4848, 'percent_change_1h': 0.01408852, 'percent_change_24h': -0.0003398, 'percent_change_7d': 0.00730438, 'percent_change_30d': 0.05249274, 'percent_change_60d': 0.00242843, 'percent_change_90d': 0.00285148, 'market_cap': 83204729139.86082, 'market_cap_dominance': 7.7686, 'fully_diluted_market_cap': 86440444556.48, 'tvl': None, 'last_updated': '2023-09-21T04:05:00.000Z'}}}, {'id': 1839, 'name': 'BNB', 'symbol': 'BNB', 'slug': 'bnb', 'num_market_pairs': 1631, 'date_added': '2017-07-25T00:00:00.000Z', 'tags': ['marketplace', 'centralized-exchange', 'payments', 'smart-contracts', 'alameda-research-portfolio', 'multicoin-capital-portfolio', 'bnb-chain', 'layer-1', 'sec-security-token', 'alleged-sec-securities', 'celsius-bankruptcy-estate'], 'max_supply': None, 'circulating_supply': 153847800.54376468, 'total_supply': 153847800.54376468, 'infinite_supply': False, 'platform': None, 'cmc_rank': 4, 'self_reported_circulating_supply': None, 'self_reported_market_cap': None, 'tvl_ratio': None, 'last_updated': '2023-09-21T04:05:00.000Z', 'quote': {'USD': {'price': 214.970318640548, 'volume_24h': 382337209.30940676, 'volume_change_24h': -0.2239, 'percent_change_1h': 0.25947663, 'percent_change_24h': -0.62262213, 'percent_change_7d': 1.37262744, 'percent_change_30d': 2.33489852, 'percent_change_60d': -11.15155726, 'percent_change_90d': -11.12640875, 'market_cap': 33072710705.040565, 'market_cap_dominance': 3.0879, 'fully_diluted_market_cap': 33072710705.04, 'tvl': None, 'last_updated': '2023-09-21T04:05:00.000Z'}}}, {'id': 52, 'name': 'XRP', 'symbol': 'XRP', 'slug': 'xrp', 'num_market_pairs': 1096, 'date_added': '2013-08-04T00:00:00.000Z', 'tags': ['medium-of-exchange', 'enterprise-solutions', 'arrington-xrp-capital-portfolio', 'galaxy-digital-portfolio', 'a16z-portfolio', 'pantera-capital-portfolio', 'ftx-bankruptcy-estate'], 'max_supply': 100000000000, 'circulating_supply': 53175400720, 'total_supply': 99988428101, 'infinite_supply': False, 'platform': None, 'cmc_rank': 5, 'self_reported_circulating_supply': None, 'self_reported_market_cap': None, 'tvl_ratio': None, 'last_updated': '2023-09-21T04:05:00.000Z', 'quote': {'USD': {'price': 0.517976149329635, 'volume_24h': 1066146221.9165273, 'volume_change_24h': 9.0557, 'percent_change_1h': 0.26080346, 'percent_change_24h': -0.92112913, 'percent_change_7d': 6.85007809, 'percent_change_30d': -0.65984975, 'percent_change_60d': -29.726557, 'percent_change_90d': 3.4674249, 'market_cap': 27543589304.0059, 'market_cap_dominance': 2.5717, 'fully_diluted_market_cap': 51797614932.96, 'tvl': None, 'last_updated': '2023-09-21T04:05:00.000Z'}}}]\n",
      "BTC 27021.62556040232\n",
      "ETH 1622.5451268449574\n",
      "USDT 1.0001648323106935\n",
      "BNB 214.970318640548\n",
      "XRP 0.517976149329635\n"
     ]
    }
   ],
   "source": [
    "!python bitcoin.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E002] Can't find factory for 'sentiment' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components).\n\nAvailable factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, doc_cleaner, parser, beam_parser, lemmatizer, trainable_lemmatizer, entity_linker, entity_ruler, tagger, morphologizer, ner, beam_ner, senter, sentencizer, spancat, spancat_singlelabel, span_finder, future_entity_ruler, span_ruler, textcat, textcat_multilabel, en.lemmatizer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\OneDrive\\Documents\\SE_Factory\\FSD\\Tech\\data_analysis_saturn_group\\notebook.ipynb Cell 4\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/Documents/SE_Factory/FSD/Tech/data_analysis_saturn_group/notebook.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/Documents/SE_Factory/FSD/Tech/data_analysis_saturn_group/notebook.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39men_core_web_sm\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/Documents/SE_Factory/FSD/Tech/data_analysis_saturn_group/notebook.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m nlp\u001b[39m.\u001b[39;49madd_pipe(\u001b[39m\"\u001b[39;49m\u001b[39msentiment\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/Documents/SE_Factory/FSD/Tech/data_analysis_saturn_group/notebook.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39manalyze_sentiment\u001b[39m(text):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/Documents/SE_Factory/FSD/Tech/data_analysis_saturn_group/notebook.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m   doc \u001b[39m=\u001b[39m nlp(text)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\sefactory_env\\lib\\site-packages\\spacy\\language.py:814\u001b[0m, in \u001b[0;36mLanguage.add_pipe\u001b[1;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[0;32m    810\u001b[0m     pipe_component, factory_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_pipe_from_source(\n\u001b[0;32m    811\u001b[0m         factory_name, source, name\u001b[39m=\u001b[39mname\n\u001b[0;32m    812\u001b[0m     )\n\u001b[0;32m    813\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 814\u001b[0m     pipe_component \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_pipe(\n\u001b[0;32m    815\u001b[0m         factory_name,\n\u001b[0;32m    816\u001b[0m         name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m    817\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[0;32m    818\u001b[0m         raw_config\u001b[39m=\u001b[39;49mraw_config,\n\u001b[0;32m    819\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[0;32m    820\u001b[0m     )\n\u001b[0;32m    821\u001b[0m pipe_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_pipe_index(before, after, first, last)\n\u001b[0;32m    822\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pipe_meta[name] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_factory_meta(factory_name)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\sefactory_env\\lib\\site-packages\\spacy\\language.py:683\u001b[0m, in \u001b[0;36mLanguage.create_pipe\u001b[1;34m(self, factory_name, name, config, raw_config, validate)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhas_factory(factory_name):\n\u001b[0;32m    676\u001b[0m     err \u001b[39m=\u001b[39m Errors\u001b[39m.\u001b[39mE002\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    677\u001b[0m         name\u001b[39m=\u001b[39mfactory_name,\n\u001b[0;32m    678\u001b[0m         opts\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfactory_names),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    681\u001b[0m         lang_code\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlang,\n\u001b[0;32m    682\u001b[0m     )\n\u001b[1;32m--> 683\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(err)\n\u001b[0;32m    684\u001b[0m pipe_meta \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_factory_meta(factory_name)\n\u001b[0;32m    685\u001b[0m \u001b[39m# This is unideal, but the alternative would mean you always need to\u001b[39;00m\n\u001b[0;32m    686\u001b[0m \u001b[39m# specify the full config settings, which is not really viable.\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: [E002] Can't find factory for 'sentiment' for language English (en). This usually happens when spaCy calls `nlp.create_pipe` with a custom component name that's not registered on the current language class. If you're using a Transformer, make sure to install 'spacy-transformers'. If you're using a custom component, make sure you've added the decorator `@Language.component` (for function components) or `@Language.factory` (for class components).\n\nAvailable factories: attribute_ruler, tok2vec, merge_noun_chunks, merge_entities, merge_subtokens, token_splitter, doc_cleaner, parser, beam_parser, lemmatizer, trainable_lemmatizer, entity_linker, entity_ruler, tagger, morphologizer, ner, beam_ner, senter, sentencizer, spancat, spancat_singlelabel, span_finder, future_entity_ruler, span_ruler, textcat, textcat_multilabel, en.lemmatizer"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"sentiment\")\n",
    "def analyze_sentiment(text):\n",
    "  \n",
    "  doc = nlp(text)\n",
    "  sentiment_scores = doc._.sentiment\n",
    "  compound_score = sentiment_scores[\"compound\"]\n",
    "\n",
    "  if compound_score >= 0.05:\n",
    "    sentiment = \"positive\"\n",
    "  elif compound_score <= -0.05:\n",
    "    sentiment = \"negative\"\n",
    "  else:\n",
    "    sentiment = \"neutral\"\n",
    "\n",
    "  return sentiment, compound_score\n",
    "\n",
    "texts = [\n",
    "  \"I love this product! It's amazing.\",\n",
    "  \"This movie is terrible.\",\n",
    "  \"The weather today is okay.\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "  sentiment, compound_score = analyze_sentiment(text)\n",
    "  print(f\"Text: {text}\")\n",
    "  print(f\"Sentiment: {sentiment} (Compound Score: {compound_score})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.7167999148368835}]\n",
      "Sentiment Label: LABEL_0\n",
      "Sentiment Score: 0.7167999148368835\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load a pre-trained transformer model for sentiment analysis (e.g., BERT)\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "# Example text\n",
    "text = \"The weather today is okay.\"\n",
    "\n",
    "# Tokenize the text using spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Convert spaCy tokens back to a text string\n",
    "tokenized_text = \" \".join([token.text for token in doc])\n",
    "\n",
    "\n",
    "# Perform sentiment analysis using the transformer model\n",
    "sentiment_results = sentiment_analysis(tokenized_text)\n",
    "\n",
    "# Extract the sentiment label and score\n",
    "sentiment_label = sentiment_results[0][\"label\"]\n",
    "sentiment_score = sentiment_results[0][\"score\"]\n",
    "\n",
    "print(sentiment_results)\n",
    "print(f\"Sentiment Label: {sentiment_label}\")\n",
    "print(f\"Sentiment Score: {sentiment_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I love this product! It's amazing.\n",
      "Sentiment: LABEL_1 (Compound Score: 0.6537538170814514)\n",
      "Text: This movie is terrible.\n",
      "Sentiment: LABEL_1 (Compound Score: 0.6499747037887573)\n",
      "Text: The weather today is okay.\n",
      "Sentiment: LABEL_1 (Compound Score: 0.6475648880004883)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"bert-base-uncased\")\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "  \n",
    "  doc = nlp(text)\n",
    "  tokenized_text = \" \".join([token.text for token in doc])\n",
    "  sentiment_results = sentiment_analysis(tokenized_text)\n",
    "  # sentiment_scores = doc._.sentiment\n",
    "  sentiment_label = sentiment_results[0][\"label\"]\n",
    "  sentiment_score = sentiment_results[0][\"score\"]\n",
    "\n",
    "  # compound_score = sentiment_scores[\"compound\"]\n",
    "\n",
    "  # if compound_score >= 0.05:\n",
    "  #   sentiment = \"positive\"\n",
    "  # elif compound_score <= -0.05:\n",
    "  #   sentiment = \"negative\"\n",
    "  # else:\n",
    "  #   sentiment = \"neutral\"\n",
    "\n",
    "  return sentiment_label, sentiment_score\n",
    "\n",
    "texts = [\n",
    "  \"I love this product! It's amazing.\",\n",
    "  \"This movie is terrible.\",\n",
    "  \"The weather today is okay.\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "  # sentiment, compound_score = analyze_sentiment(text)\n",
    "  sentiment, compound_score = analyze_sentiment(text)\n",
    "\n",
    "  print(f\"Text: {text}\")\n",
    "  print(f\"Sentiment: {sentiment} (Compound Score: {compound_score})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.9932832717895508}]\n",
      "Sentiment Label: LABEL_1\n",
      "Sentiment Score: 0.9932832717895508\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load a pre-trained transformer model for sentiment analysis (e.g., GPT2)\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"gpt2\")\n",
    "\n",
    "\n",
    "# Example text\n",
    "text = \"This movie is terrible.\"\n",
    "\n",
    "# Tokenize the text using spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Convert spaCy tokens back to a text string\n",
    "tokenized_text = \" \".join([token.text for token in doc])\n",
    "\n",
    "\n",
    "# Perform sentiment analysis using the transformer model\n",
    "sentiment_results = sentiment_analysis(tokenized_text)\n",
    "\n",
    "# Extract the sentiment label and score\n",
    "sentiment_label = sentiment_results[0][\"label\"]\n",
    "sentiment_score = sentiment_results[0][\"score\"]\n",
    "\n",
    "print(sentiment_results)\n",
    "print(f\"Sentiment Label: {sentiment_label}\")\n",
    "print(f\"Sentiment Score: {sentiment_score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)neration_config.json: 100%|██████████| 124/124 [00:00<?, ?B/s] \n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "c:\\Users\\user\\anaconda3\\envs\\sefactory_env\\lib\\site-packages\\transformers\\generation\\utils.py:1353: UserWarning: Using `max_length`'s default (50) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "text_generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "def analyze_sentiment_with_gpt2(text):\n",
    "    # Define prompts for different sentiments\n",
    "    positive_prompt = \"This movie is wonderful\"\n",
    "    negative_prompt = \"This movie is terrible\"\n",
    "\n",
    "    # Generate text with GPT-2 based on input text\n",
    "    generated_text_positive = text_generator(positive_prompt + text)[0][\"generated_text\"]\n",
    "    generated_text_negative = text_generator(negative_prompt + text)[0][\"generated_text\"]\n",
    "\n",
    "    # Analyze the generated text to infer sentiment\n",
    "    positive_score = analyze_sentiment(generated_text_positive)\n",
    "    negative_score = analyze_sentiment(generated_text_negative)\n",
    "\n",
    "    # Determine overall sentiment based on the scores\n",
    "    if positive_score > negative_score:\n",
    "        sentiment = \"Positive\"\n",
    "    elif positive_score < negative_score:\n",
    "        sentiment = \"Negative\"\n",
    "    else:\n",
    "        sentiment = \"Neutral\"\n",
    "\n",
    "    return sentiment\n",
    "\n",
    "# Example text\n",
    "text = \"I really enjoyed the movie. It was fantastic!\"\n",
    "\n",
    "# Analyze sentiment using GPT-2\n",
    "sentiment = analyze_sentiment_with_gpt2(text)\n",
    "print(f\"Sentiment: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column is not a date format=Parser must be a string or character stream, not float64\n",
      "column is not a date format=Unknown string format: @PPathole Dojo isnât needed, but will make self-driving better. It isnât enough to be safer than human drivers, Autopilot ultimately needs to be more than 10 times safer than human drivers. present at position 0\n",
      "column is not a date format=Parser must be a string or character stream, not int64\n",
      "column is not a date format=String does not contain a date: Aave\n",
      "column is not a date format=String does not contain a date: AAVE\n",
      "column is not a date format=Parser must be a string or character stream, not float64\n",
      "column is not a date format=Parser must be a string or character stream, not float64\n",
      "column is not a date format=Parser must be a string or character stream, not float64\n",
      "column is not a date format=Parser must be a string or character stream, not float64\n",
      "column is not a date format=Parser must be a string or character stream, not float64\n",
      "column is not a date format=Parser must be a string or character stream, not float64\n",
      "column is not a date format=Parser must be a string or character stream, not int64\n",
      "column is not a date format=String does not contain a date: Binance Coin\n",
      "column is not a date format=String does not contain a date: BNB\n",
      "column is not a date format=Parser must be a string or character stream, not float64\n",
      "column is not a date format=Parser must be a string or character stream, not float64\n",
      "column is not a date format=Parser must be a string or character stream, not float64\n",
      "column is not a date format=Parser must be a string or character stream, not float64\n",
      "column is not a date format=Parser must be a string or character stream, not float64\n",
      "column is not a date format=Parser must be a string or character stream, not float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tweet Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.344810e+18</th>\n",
       "      <td>2021-01-01 00:58:00</td>\n",
       "      <td>@PPathole Dojo isnât needed, but will make s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.345210e+18</th>\n",
       "      <td>2021-02-01 03:20:00</td>\n",
       "      <td>@comma_ai Tesla Full Self-Driving will work at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.345340e+18</th>\n",
       "      <td>2021-02-01 12:23:00</td>\n",
       "      <td>@newscientist Um, we have giant fusion reactor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.345380e+18</th>\n",
       "      <td>2021-02-01 14:51:00</td>\n",
       "      <td>So proud of the Tesla team for achieving this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.345380e+18</th>\n",
       "      <td>2021-02-01 14:59:00</td>\n",
       "      <td>@flcnhvy Tesla is responsible for 2/3 of all t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.476620e+18</th>\n",
       "      <td>2021-12-30 18:23:00</td>\n",
       "      <td>@BLKMDL3 @mims Predicting macroeconomics is ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.476650e+18</th>\n",
       "      <td>2021-12-30 20:28:00</td>\n",
       "      <td>@CSmithson80 @heydave7 @BLKMDL3 @mims This cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.476660e+18</th>\n",
       "      <td>2021-12-30 20:47:00</td>\n",
       "      <td>@tesla_raj Many UI improvements coming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.476660e+18</th>\n",
       "      <td>2021-12-30 21:11:00</td>\n",
       "      <td>@roshanpateI ð¤£ $7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.476740e+18</th>\n",
       "      <td>2021-12-31 02:23:00</td>\n",
       "      <td>@TheBabylonBee ð¤£</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2993 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Datetime  \\\n",
       "Tweet Id                           \n",
       "1.344810e+18 2021-01-01 00:58:00   \n",
       "1.345210e+18 2021-02-01 03:20:00   \n",
       "1.345340e+18 2021-02-01 12:23:00   \n",
       "1.345380e+18 2021-02-01 14:51:00   \n",
       "1.345380e+18 2021-02-01 14:59:00   \n",
       "...                          ...   \n",
       "1.476620e+18 2021-12-30 18:23:00   \n",
       "1.476650e+18 2021-12-30 20:28:00   \n",
       "1.476660e+18 2021-12-30 20:47:00   \n",
       "1.476660e+18 2021-12-30 21:11:00   \n",
       "1.476740e+18 2021-12-31 02:23:00   \n",
       "\n",
       "                                                           Text  \n",
       "Tweet Id                                                         \n",
       "1.344810e+18  @PPathole Dojo isnât needed, but will make s...  \n",
       "1.345210e+18  @comma_ai Tesla Full Self-Driving will work at...  \n",
       "1.345340e+18  @newscientist Um, we have giant fusion reactor...  \n",
       "1.345380e+18  So proud of the Tesla team for achieving this ...  \n",
       "1.345380e+18  @flcnhvy Tesla is responsible for 2/3 of all t...  \n",
       "...                                                         ...  \n",
       "1.476620e+18  @BLKMDL3 @mims Predicting macroeconomics is ch...  \n",
       "1.476650e+18  @CSmithson80 @heydave7 @BLKMDL3 @mims This cha...  \n",
       "1.476660e+18             @tesla_raj Many UI improvements coming  \n",
       "1.476660e+18                               @roshanpateI ð¤£ $7  \n",
       "1.476740e+18                                @TheBabylonBee ð¤£  \n",
       "\n",
       "[2993 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lookups import CsvUrlTweets,CsvUrlHistoricalData\n",
    "from pandas_data_handler import get_online_csv_into_df_list\n",
    "\n",
    "df_list,df_titles = get_online_csv_into_df_list(CsvUrlTweets,CsvUrlHistoricalData)\n",
    "df_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Date</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Marketcap</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SNo</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aave</td>\n",
       "      <td>AAVE</td>\n",
       "      <td>2020-10-05 23:59:59</td>\n",
       "      <td>55.112358</td>\n",
       "      <td>49.7879</td>\n",
       "      <td>52.675035</td>\n",
       "      <td>53.219243</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.912813e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Name Symbol                Date       High      Low       Open  \\\n",
       "SNo                                                                   \n",
       "1    Aave   AAVE 2020-10-05 23:59:59  55.112358  49.7879  52.675035   \n",
       "\n",
       "         Close  Volume     Marketcap  \n",
       "SNo                                   \n",
       "1    53.219243     0.0  8.912813e+07  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list[1].iloc[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column is not a date format=Parser must be a string or character stream, not float64\n",
      "column is not a date format=Unknown string format: @PPathole Dojo isnât needed, but will make self-driving better. It isnât enough to be safer than human drivers, Autopilot ultimately needs to be more than 10 times safer than human drivers. present at position 0\n",
      "column is not a date format=Parser must be a string or character stream, not int64\n",
      "column is not a date format=String does not contain a date: Aave\n",
      "column is not a date format=String does not contain a date: AAVE\n",
      "column is not a date format=Parser must be a string or character stream, not float64\n",
      "column is not a date format=Parser must be a string or character stream, not float64\n",
      "column is not a date format=Parser must be a string or character stream, not float64\n",
      "column is not a date format=Parser must be a string or character stream, not float64\n",
      "column is not a date format=Parser must be a string or character stream, not float64\n",
      "column is not a date format=Parser must be a string or character stream, not float64\n",
      "column is not a date format=Parser must be a string or character stream, not int64\n",
      "column is not a date format=String does not contain a date: Binance Coin\n",
      "column is not a date format=String does not contain a date: BNB\n",
      "column is not a date format=Parser must be a string or character stream, not float64\n",
      "column is not a date format=Parser must be a string or character stream, not float64\n",
      "column is not a date format=Parser must be a string or character stream, not float64\n",
      "column is not a date format=Parser must be a string or character stream, not float64\n",
      "column is not a date format=Parser must be a string or character stream, not float64\n",
      "column is not a date format=Parser must be a string or character stream, not float64\n"
     ]
    }
   ],
   "source": [
    "from prehook import execute_prehook\n",
    "\n",
    "execute_prehook()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sefactory_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
