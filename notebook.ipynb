{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Datetime      Tweet Id  \\\n",
      "0  01/01/2021 00:58  1.344810e+18   \n",
      "1  02/01/2021 03:20  1.345210e+18   \n",
      "2  02/01/2021 12:23  1.345340e+18   \n",
      "3  02/01/2021 14:51  1.345380e+18   \n",
      "4  02/01/2021 14:59  1.345380e+18   \n",
      "\n",
      "                                                Text  \n",
      "0  @PPathole Dojo isnât needed, but will make s...  \n",
      "1  @comma_ai Tesla Full Self-Driving will work at...  \n",
      "2  @newscientist Um, we have giant fusion reactor...  \n",
      "3  So proud of the Tesla team for achieving this ...  \n",
      "4  @flcnhvy Tesla is responsible for 2/3 of all t...  \n"
     ]
    }
   ],
   "source": [
    "from pandas_data_handler import download_csv_to_dataframe,download_and_convert_csv,download_csv_to_dataframe_2\n",
    "\n",
    "# Example usage:\n",
    "csv_url = 'https://storage.googleapis.com/kagglesdsdata/datasets/3563626/6206522/Emusk_2021_tweets.csv?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20230920%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20230920T144512Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=33c16d7667b7d7b016856e3955505b7261e058d88c026b6586cba7c78b30feda5839a21220bd5d29a69e73a978711eac8d92d908cc08005c72ad6535a9a545f64f852e814f8fcf89e9468834fbfcfe7119470bcc1c119553b86e3f8c6554b3df4f83a34df0558904041585a32c9f0ad5b349558e78f15b25eb2179a842feec84c90c2eade403058bec18c5b3491460fbccd91d166d9f912ec36599b3cc7cef9fe7945d98fc3ba56adf75c27af5b5e7a2838408da40e4692f7603ecf5ccbd27f1ac347291fa23b6be83e8581828915be54f02651fed30ff1d70a02befa2d39138d80205218f0424b274b6119b40f8d94b31d0cb7f8721144511e2df5434dc25a0' \n",
    "data_frame = download_csv_to_dataframe_2(csv_url)\n",
    "\n",
    "if data_frame is not None:\n",
    "    print(data_frame.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: File is not a zip file\n"
     ]
    }
   ],
   "source": [
    "csv_url = 'https://www.kaggle.com/datasets/johnsmith44/dogecoin-price-data-elon-musks-tweets-2021/download?datasetVersionNumber=3' \n",
    "data_frame = download_and_convert_csv(csv_url,save_path='downloaded_zip_file')\n",
    "\n",
    "if data_frame:\n",
    "    print(data_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Replace this URL with the Kaggle dataset URL you want to scrape\n",
    "url = \"https://www.kaggle.com/datasets/johnsmith44/dogecoin-price-data-elon-musks-tweets-2021\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    html_content = response.text\n",
    "else:\n",
    "    print(\"Failed to fetch the page. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find the table element containing the dataset information\n",
    "a = soup.find('a')\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table not found on the page.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if table:\n",
    "    # Extract the table data into a DataFrame\n",
    "    df = pd.read_html(str(table))[0]\n",
    "\n",
    "    # You can now work with 'df', which contains the dataset information as a DataFrame.\n",
    "    print(df.head())  # Display the first few rows of the DataFrame\n",
    "else:\n",
    "    print(\"Table not found on the page.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sefactory_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
