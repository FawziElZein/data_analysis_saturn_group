{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": null,
>>>>>>> 09b00b72d68b885fe2b0bc23c3ad96f7d5f3ea14
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import NaN\n",
    "import spacy\n",
    "from transformers import pipeline\n",
<<<<<<< HEAD
    "import nltk\n",
    "import re\n",
=======
    "import re\n",
    "import nltk\n",
>>>>>>> 09b00b72d68b885fe2b0bc23c3ad96f7d5f3ea14
    "from textblob import TextBlob\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from yahoofinancials import YahooFinancials"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing function\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
>>>>>>> 09b00b72d68b885fe2b0bc23c3ad96f7d5f3ea14
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
=======
   "execution_count": null,
>>>>>>> 09b00b72d68b885fe2b0bc23c3ad96f7d5f3ea14
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # remove punctuation and special characters\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        # convert to lowercase\n",
    "        text = text.lower()\n",
    "        # tokenize text\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        # remove stop words\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        # lemmatize text\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        # join tokens back into text\n",
    "        text = ' '.join(tokens)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset_52-person-from-2021-02-05_2023-06-12_21-34-17-266_with_sentiment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>full_text</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>importance_coefficient</th>\n",
       "      <th>importance_coefficient_normalized</th>\n",
       "      <th>new_coins</th>\n",
       "      <th>scores</th>\n",
       "      <th>compound</th>\n",
       "      <th>sentiment_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32666</td>\n",
       "      <td>2/1/2021</td>\n",
       "      <td>154</td>\n",
       "      <td>#privacy is a human right. learn how to make y...</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>340.0</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>(bitcoin)</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29639</td>\n",
       "      <td>2/1/2021</td>\n",
       "      <td>17</td>\n",
       "      <td>overall btc trading volume has increased, but ...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>39.5</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>(btc)</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.95, 'pos': 0.05, 'compou...</td>\n",
       "      <td>0.2124</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29613</td>\n",
       "      <td>2/1/2021</td>\n",
       "      <td>3</td>\n",
       "      <td>on average, the return distribution of btc ske...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>(btc)</td>\n",
       "      <td>{'neg': 0.053, 'neu': 0.769, 'pos': 0.177, 'co...</td>\n",
       "      <td>0.7010</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39638</td>\n",
       "      <td>2/1/2021</td>\n",
       "      <td>3496</td>\n",
       "      <td>i sent some! https://t.co/mfyrz35zjf\\n\\nyou sh...</td>\n",
       "      <td>731</td>\n",
       "      <td>686</td>\n",
       "      <td>8043.5</td>\n",
       "      <td>0.013905</td>\n",
       "      <td>(doge)</td>\n",
       "      <td>{'neg': 0.06, 'neu': 0.856, 'pos': 0.084, 'com...</td>\n",
       "      <td>0.2225</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32660</td>\n",
       "      <td>2/1/2021</td>\n",
       "      <td>0</td>\n",
       "      <td>rt @reg_mati: la privacidad es un derecho huma...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>(bitcoin)</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16507</th>\n",
       "      <td>1309</td>\n",
       "      <td>6/12/2023</td>\n",
       "      <td>3</td>\n",
       "      <td>booomð¥\\n\\nour #ai bot/indicator crushes ano...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>(doge,hbar,inj,usdt,matic,ftm)</td>\n",
       "      <td>{'neg': 0.092, 'neu': 0.75, 'pos': 0.158, 'com...</td>\n",
       "      <td>0.6239</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16508</th>\n",
       "      <td>1307</td>\n",
       "      <td>6/12/2023</td>\n",
       "      <td>0</td>\n",
       "      <td>rt @crypto_crib_: the deadline is today for bi...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>(binance,request)</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16509</th>\n",
       "      <td>1306</td>\n",
       "      <td>6/12/2023</td>\n",
       "      <td>0</td>\n",
       "      <td>rt @crypto_crib_: ð²chinese bank boci issues...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>(ethereum)</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.844, 'pos': 0.156, 'comp...</td>\n",
       "      <td>0.3400</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16510</th>\n",
       "      <td>1305</td>\n",
       "      <td>6/12/2023</td>\n",
       "      <td>56</td>\n",
       "      <td>bitcoin, not crypto.\\n\\ncrypto, not security.</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>127.0</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>(bitcoin)</td>\n",
       "      <td>{'neg': 0.289, 'neu': 0.711, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.2584</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16511</th>\n",
       "      <td>1312</td>\n",
       "      <td>6/12/2023</td>\n",
       "      <td>0</td>\n",
       "      <td>rt @crypto_crib_: jpmorgan calls for \"comprehe...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>(binance,amp)</td>\n",
       "      <td>{'neg': 0.084, 'neu': 0.752, 'pos': 0.164, 'co...</td>\n",
       "      <td>0.2023</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16512 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0 created_at  favorite_count  \\\n",
       "0           32666   2/1/2021             154   \n",
       "1           29639   2/1/2021              17   \n",
       "2           29613   2/1/2021               3   \n",
       "3           39638   2/1/2021            3496   \n",
       "4           32660   2/1/2021               0   \n",
       "...           ...        ...             ...   \n",
       "16507        1309  6/12/2023               3   \n",
       "16508        1307  6/12/2023               0   \n",
       "16509        1306  6/12/2023               0   \n",
       "16510        1305  6/12/2023              56   \n",
       "16511        1312  6/12/2023               0   \n",
       "\n",
       "                                               full_text  reply_count  \\\n",
       "0      #privacy is a human right. learn how to make y...           18   \n",
       "1      overall btc trading volume has increased, but ...            1   \n",
       "2      on average, the return distribution of btc ske...            0   \n",
       "3      i sent some! https://t.co/mfyrz35zjf\\n\\nyou sh...          731   \n",
       "4      rt @reg_mati: la privacidad es un derecho huma...            0   \n",
       "...                                                  ...          ...   \n",
       "16507  booomð¥\\n\\nour #ai bot/indicator crushes ano...            5   \n",
       "16508  rt @crypto_crib_: the deadline is today for bi...            0   \n",
       "16509  rt @crypto_crib_: ð²chinese bank boci issues...            0   \n",
       "16510      bitcoin, not crypto.\\n\\ncrypto, not security.           16   \n",
       "16511  rt @crypto_crib_: jpmorgan calls for \"comprehe...            0   \n",
       "\n",
       "       retweet_count  importance_coefficient  \\\n",
       "0                 23                   340.0   \n",
       "1                  5                    39.5   \n",
       "2                  1                     7.0   \n",
       "3                686                  8043.5   \n",
       "4                  7                     7.0   \n",
       "...              ...                     ...   \n",
       "16507              0                     8.5   \n",
       "16508              4                     4.0   \n",
       "16509              8                     8.0   \n",
       "16510              7                   127.0   \n",
       "16511              8                     8.0   \n",
       "\n",
       "       importance_coefficient_normalized                       new_coins  \\\n",
       "0                               0.000588                       (bitcoin)   \n",
       "1                               0.000068                           (btc)   \n",
       "2                               0.000012                           (btc)   \n",
       "3                               0.013905                          (doge)   \n",
       "4                               0.000012                       (bitcoin)   \n",
       "...                                  ...                             ...   \n",
       "16507                           0.000015  (doge,hbar,inj,usdt,matic,ftm)   \n",
       "16508                           0.000007               (binance,request)   \n",
       "16509                           0.000014                      (ethereum)   \n",
       "16510                           0.000220                       (bitcoin)   \n",
       "16511                           0.000014                   (binance,amp)   \n",
       "\n",
       "                                                  scores  compound  \\\n",
       "0      {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...    0.0000   \n",
       "1      {'neg': 0.0, 'neu': 0.95, 'pos': 0.05, 'compou...    0.2124   \n",
       "2      {'neg': 0.053, 'neu': 0.769, 'pos': 0.177, 'co...    0.7010   \n",
       "3      {'neg': 0.06, 'neu': 0.856, 'pos': 0.084, 'com...    0.2225   \n",
       "4      {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...    0.0000   \n",
       "...                                                  ...       ...   \n",
       "16507  {'neg': 0.092, 'neu': 0.75, 'pos': 0.158, 'com...    0.6239   \n",
       "16508  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...    0.0000   \n",
       "16509  {'neg': 0.0, 'neu': 0.844, 'pos': 0.156, 'comp...    0.3400   \n",
       "16510  {'neg': 0.289, 'neu': 0.711, 'pos': 0.0, 'comp...   -0.2584   \n",
       "16511  {'neg': 0.084, 'neu': 0.752, 'pos': 0.164, 'co...    0.2023   \n",
       "\n",
       "      sentiment_type  \n",
       "0            NEUTRAL  \n",
       "1           POSITIVE  \n",
       "2           POSITIVE  \n",
       "3           POSITIVE  \n",
       "4            NEUTRAL  \n",
       "...              ...  \n",
       "16507       POSITIVE  \n",
       "16508        NEUTRAL  \n",
       "16509       POSITIVE  \n",
       "16510       NEGATIVE  \n",
       "16511       POSITIVE  \n",
       "\n",
       "[16512 rows x 12 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>full_text</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>user_id</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>new_coins</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32666</td>\n",
       "      <td>2/1/2021</td>\n",
       "      <td>154</td>\n",
       "      <td>#privacy is a human right. learn how to make y...</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>aantonop</td>\n",
       "      <td>privacy human right learn make bitcoin transac...</td>\n",
       "      <td>(bitcoin)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29639</td>\n",
       "      <td>2/1/2021</td>\n",
       "      <td>17</td>\n",
       "      <td>overall btc trading volume has increased, but ...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>CoinDeskData</td>\n",
       "      <td>overall btc trading volume increased average t...</td>\n",
       "      <td>(btc)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29613</td>\n",
       "      <td>2/1/2021</td>\n",
       "      <td>3</td>\n",
       "      <td>on average, the return distribution of btc ske...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>CoinDeskData</td>\n",
       "      <td>average return distribution btc skews slightly...</td>\n",
       "      <td>(btc)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39638</td>\n",
       "      <td>2/1/2021</td>\n",
       "      <td>3496</td>\n",
       "      <td>i sent some! https://t.co/mfyrz35zjf\\n\\nyou sh...</td>\n",
       "      <td>731</td>\n",
       "      <td>686</td>\n",
       "      <td>VitalikButerin</td>\n",
       "      <td>sent httpstcomfyrz35zjf givedirectly great wor...</td>\n",
       "      <td>(doge)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32660</td>\n",
       "      <td>2/1/2021</td>\n",
       "      <td>0</td>\n",
       "      <td>rt @reg_mati: la privacidad es un derecho huma...</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>aantonop</td>\n",
       "      <td>rt reg_mati la privacidad e un derecho humano ...</td>\n",
       "      <td>(bitcoin)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16507</th>\n",
       "      <td>1309</td>\n",
       "      <td>6/12/2023</td>\n",
       "      <td>3</td>\n",
       "      <td>booomð¥\\n\\nour #ai bot/indicator crushes ano...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>BinanceKiller</td>\n",
       "      <td>booomð ai botindicator crush another lina trad...</td>\n",
       "      <td>(doge,hbar,inj,usdt,matic,ftm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16508</th>\n",
       "      <td>1307</td>\n",
       "      <td>6/12/2023</td>\n",
       "      <td>0</td>\n",
       "      <td>rt @crypto_crib_: the deadline is today for bi...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>AltcoinGordon</td>\n",
       "      <td>rt crypto_crib_ deadline today binance binance...</td>\n",
       "      <td>(binance,request)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16509</th>\n",
       "      <td>1306</td>\n",
       "      <td>6/12/2023</td>\n",
       "      <td>0</td>\n",
       "      <td>rt @crypto_crib_: ð²chinese bank boci issues...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>AltcoinGordon</td>\n",
       "      <td>rt crypto_crib_ ð²chinese bank boci issue coun...</td>\n",
       "      <td>(ethereum)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16510</th>\n",
       "      <td>1305</td>\n",
       "      <td>6/12/2023</td>\n",
       "      <td>56</td>\n",
       "      <td>bitcoin, not crypto.\\n\\ncrypto, not security.</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>AltcoinGordon</td>\n",
       "      <td>bitcoin crypto crypto security</td>\n",
       "      <td>(bitcoin)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16511</th>\n",
       "      <td>1312</td>\n",
       "      <td>6/12/2023</td>\n",
       "      <td>0</td>\n",
       "      <td>rt @crypto_crib_: jpmorgan calls for \"comprehe...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>AltcoinGordon</td>\n",
       "      <td>rt crypto_crib_ jpmorgan call comprehensive fr...</td>\n",
       "      <td>(binance,amp)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16512 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0 created_at  favorite_count  \\\n",
       "0           32666   2/1/2021             154   \n",
       "1           29639   2/1/2021              17   \n",
       "2           29613   2/1/2021               3   \n",
       "3           39638   2/1/2021            3496   \n",
       "4           32660   2/1/2021               0   \n",
       "...           ...        ...             ...   \n",
       "16507        1309  6/12/2023               3   \n",
       "16508        1307  6/12/2023               0   \n",
       "16509        1306  6/12/2023               0   \n",
       "16510        1305  6/12/2023              56   \n",
       "16511        1312  6/12/2023               0   \n",
       "\n",
       "                                               full_text  reply_count  \\\n",
       "0      #privacy is a human right. learn how to make y...           18   \n",
       "1      overall btc trading volume has increased, but ...            1   \n",
       "2      on average, the return distribution of btc ske...            0   \n",
       "3      i sent some! https://t.co/mfyrz35zjf\\n\\nyou sh...          731   \n",
       "4      rt @reg_mati: la privacidad es un derecho huma...            0   \n",
       "...                                                  ...          ...   \n",
       "16507  booomð¥\\n\\nour #ai bot/indicator crushes ano...            5   \n",
       "16508  rt @crypto_crib_: the deadline is today for bi...            0   \n",
       "16509  rt @crypto_crib_: ð²chinese bank boci issues...            0   \n",
       "16510      bitcoin, not crypto.\\n\\ncrypto, not security.           16   \n",
       "16511  rt @crypto_crib_: jpmorgan calls for \"comprehe...            0   \n",
       "\n",
       "       retweet_count         user_id  \\\n",
       "0                 23        aantonop   \n",
       "1                  5    CoinDeskData   \n",
       "2                  1    CoinDeskData   \n",
       "3                686  VitalikButerin   \n",
       "4                  7        aantonop   \n",
       "...              ...             ...   \n",
       "16507              0   BinanceKiller   \n",
       "16508              4   AltcoinGordon   \n",
       "16509              8   AltcoinGordon   \n",
       "16510              7   AltcoinGordon   \n",
       "16511              8   AltcoinGordon   \n",
       "\n",
       "                                              clean_text  \\\n",
       "0      privacy human right learn make bitcoin transac...   \n",
       "1      overall btc trading volume increased average t...   \n",
       "2      average return distribution btc skews slightly...   \n",
       "3      sent httpstcomfyrz35zjf givedirectly great wor...   \n",
       "4      rt reg_mati la privacidad e un derecho humano ...   \n",
       "...                                                  ...   \n",
       "16507  booomð ai botindicator crush another lina trad...   \n",
       "16508  rt crypto_crib_ deadline today binance binance...   \n",
       "16509  rt crypto_crib_ ð²chinese bank boci issue coun...   \n",
       "16510                     bitcoin crypto crypto security   \n",
       "16511  rt crypto_crib_ jpmorgan call comprehensive fr...   \n",
       "\n",
       "                            new_coins  \n",
       "0                           (bitcoin)  \n",
       "1                               (btc)  \n",
       "2                               (btc)  \n",
       "3                              (doge)  \n",
       "4                           (bitcoin)  \n",
       "...                               ...  \n",
       "16507  (doge,hbar,inj,usdt,matic,ftm)  \n",
       "16508               (binance,request)  \n",
       "16509                      (ethereum)  \n",
       "16510                       (bitcoin)  \n",
       "16511                   (binance,amp)  \n",
       "\n",
       "[16512 rows x 9 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the csv file into a DataFrame\n",
    "\n",
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Excel file into a DataFrame\n",
    "df = pd.read_csv(r\"C:\\Users\\Lenovo\\Desktop\\dataset_52-person-from-2021-02-05_2023-06-12_21-34-17-266_with_sentiment.csv\")\n",
>>>>>>> 09b00b72d68b885fe2b0bc23c3ad96f7d5f3ea14
    "\n",
    "# Remove rows with \"na\" values\n",
    "df = df.dropna(subset=['full_text'])\n",
    "\n",
    "# # Fill missing values in 'full_text' column with an empty string\n",
    "df['full_text'] = df['full_text'].fillna('')\n",
    "# # to lower text\n",
    "df['full_text'] = df['full_text'].str.lower()\n",
<<<<<<< HEAD
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\user/nltk_data'\n    - 'c:\\\\Users\\\\user\\\\anaconda3\\\\envs\\\\sefactory_env\\\\nltk_data'\n    - 'c:\\\\Users\\\\user\\\\anaconda3\\\\envs\\\\sefactory_env\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\user\\\\anaconda3\\\\envs\\\\sefactory_env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\OneDrive\\Documents\\SE_Factory\\FSD\\Tech\\data_analysis_saturn_group\\NLP_env.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/Documents/SE_Factory/FSD/Tech/data_analysis_saturn_group/NLP_env.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# # Preprocess the 'full_text' column\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/Documents/SE_Factory/FSD/Tech/data_analysis_saturn_group/NLP_env.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mclean_text\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mfull_text\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(preprocess_text)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\sefactory_env\\lib\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\sefactory_env\\lib\\site-packages\\pandas\\core\\apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1122\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\sefactory_env\\lib\\site-packages\\pandas\\core\\apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1173\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1174\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1175\u001b[0m             values,\n\u001b[0;32m   1176\u001b[0m             f,\n\u001b[0;32m   1177\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1178\u001b[0m         )\n\u001b[0;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1181\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\sefactory_env\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\OneDrive\\Documents\\SE_Factory\\FSD\\Tech\\data_analysis_saturn_group\\NLP_env.ipynb Cell 7\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/Documents/SE_Factory/FSD/Tech/data_analysis_saturn_group/NLP_env.ipynb#X35sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mlower()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/Documents/SE_Factory/FSD/Tech/data_analysis_saturn_group/NLP_env.ipynb#X35sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# tokenize text\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/Documents/SE_Factory/FSD/Tech/data_analysis_saturn_group/NLP_env.ipynb#X35sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m tokens \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mword_tokenize(text)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/Documents/SE_Factory/FSD/Tech/data_analysis_saturn_group/NLP_env.ipynb#X35sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# remove stop words\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/user/OneDrive/Documents/SE_Factory/FSD/Tech/data_analysis_saturn_group/NLP_env.ipynb#X35sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m tokens \u001b[39m=\u001b[39m [token \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokens \u001b[39mif\u001b[39;00m token \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m stop_words]\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\sefactory_env\\lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\sefactory_env\\lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlanguage\u001b[39m}\u001b[39;49;00m\u001b[39m.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\sefactory_env\\lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[0;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\sefactory_env\\lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\sefactory_env\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\user/nltk_data'\n    - 'c:\\\\Users\\\\user\\\\anaconda3\\\\envs\\\\sefactory_env\\\\nltk_data'\n    - 'c:\\\\Users\\\\user\\\\anaconda3\\\\envs\\\\sefactory_env\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\user\\\\anaconda3\\\\envs\\\\sefactory_env\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\user\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# # Preprocess the 'full_text' column\n",
    "df['clean_text'] = df['full_text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
=======
    "# # Preprocess the 'full_text' column\n",
    "df['clean_text'] = df['full_text'].apply(preprocess_text)\n",
>>>>>>> 09b00b72d68b885fe2b0bc23c3ad96f7d5f3ea14
    "# # Filter the DataFrame to keep rows where \"created_at\" is greater than or equal to 2021-01-01\n",
    "import datetime\n",
    "# #Convert the \"created_at\" column to datetime format\n",
    "df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "\n",
<<<<<<< HEAD
    "#add_ importance_coefficient per tweets\n",
    "\n",
=======
    "# ##add importance_coefficient per tweets\n",
>>>>>>> 09b00b72d68b885fe2b0bc23c3ad96f7d5f3ea14
    "df['importance_coefficient'] = df['retweet_count'] + 2 * df['favorite_count'] + 0.5 * df['reply_count']\n",
    "# # Find the minimum and maximum values of the importance coefficient\n",
    "min_value = df['importance_coefficient'].min()\n",
    "max_value = df['importance_coefficient'].max()\n",
    "\n",
    "# # Normalize the importance coefficient\n",
    "df['importance_coefficient_normalized'] = (df['importance_coefficient'] - min_value) / (max_value - min_value)\n",
    "# # Sort the DataFrame based on the \"created_at\" column in ascending order\n",
    "df = df.sort_values('created_at', ascending=True)\n",
    "\n",
    "# Print the sorted DataFrame\n",
    "df.head()\n",
    "df.loc[df[\"created_at\"]=='1/1/2023']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Vader Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(df):\n",
    "    # Create a copy of the input DataFrame\n",
    "    df_selected = df.copy()\n",
    "\n",
    "    # Initialize the VADER sentiment analyzer\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    # Calculate sentiment scores and add them to the DataFrame\n",
    "    df_selected['scores'] = df_selected['full_text'].apply(lambda description: sid.polarity_scores(description))\n",
    "    df_selected['compound'] = df_selected['scores'].apply(lambda score_dict: score_dict['compound'])\n",
    "    \n",
    "    # Create a new column for sentiment_type and classify based on the compound score\n",
    "    df_selected['sentiment_type'] = df_selected['compound'].apply(lambda avg_compound: 'POSITIVE' if avg_compound > 0 else 'NEUTRAL' if avg_compound == 0 else 'NEGATIVE')\n",
    "\n",
    "    return df_selected\n",
    "sentiment_df=analyze_sentiment(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
=======
>>>>>>> 09b00b72d68b885fe2b0bc23c3ad96f7d5f3ea14
    "import pandas as pd\n",
    "\n",
    "def crypto_sentiment(df, coin_symbol):\n",
    "    df = df[df[\"new_coins\"].str.contains(f\"({coin_symbol})\")].copy()\n",
    "    df = df[(df[\"created_at\"] >= \"2023-01-01\") & (df[\"created_at\"] <= \"2023-06-12\")].sort_values(by=\"created_at\", ascending=True)\n",
    "    sentiment_df = df.groupby(df['created_at'].dt.date)['compound'].mean().reset_index()\n",
    "\n",
    "    def classify_sentiment(avg_compound):\n",
    "        if avg_compound > 0:\n",
    "            return 'Positive'\n",
    "        elif avg_compound < 0:\n",
    "            return 'Negative'\n",
    "        else:\n",
    "            return 'Neutral'\n",
    "\n",
    "    sentiment_df['sentiment_type'] = sentiment_df['compound'].apply(classify_sentiment)\n",
    "\n",
    "    return sentiment_df\n",
    "\n",
    "\n",
    "\n",
    "btc_sentiment=crypto_sentiment_result = crypto_sentiment(sentiment_df, \"(btc)\")\n",
    "eth_sentiment=crypto_sentiment(sentiment_df, \"(eth)\")\n",
    "bnb_sentiment=crypto_sentiment(sentiment_df, \"(bnb)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_prices(tickers, start_date, end_date):\n",
    "    data = {}\n",
    "    for ticker in tickers:\n",
    "        yahoo_financials = YahooFinancials(ticker)\n",
    "        historical_data = yahoo_financials.get_historical_price_data(start_date, end_date, \"daily\")\n",
    "        data[ticker] = historical_data[ticker]['prices']\n",
    "    dfs = []\n",
    "    for ticker, prices in data.items():\n",
    "        df = pd.DataFrame(prices)\n",
    "        df = df.drop('date', axis=1).set_index('formatted_date')\n",
    "        df.columns = [f\"{ticker}_close\", f\"{ticker}_high\", f\"{ticker}_low\", f\"{ticker}_open\", f\"{ticker}_volume\", f\"{ticker}_adjclose\"]\n",
    "        df['formatted_date'] = pd.to_datetime(df.index) # Add formatted_date column\n",
    "        dfs.append(df)\n",
    "    merged_df = pd.concat(dfs, axis=1)\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = ['BTC-USD', 'ETH-USD', 'BNB-USD','XMR-USD','MATIC-USD','XRP-USD','DAI-USD','DOT-USD']\n",
    "start_date = '2023-01-01'\n",
    "end_date = '2023-06-12'\n",
    "\n",
    "btc_df = get_historical_prices(tickers, start_date, end_date)\n",
    "# Move formatted_date column to the first position\n",
    "btc_df = btc_df[[\"formatted_date\"] + [col for col in btc_df.columns if col != \"formatted_date\"]]\n",
    "# Delete duplicate formatted_date columns\n",
    "btc_df = btc_df.loc[:, ~btc_df.columns.duplicated()]\n",
    "print(btc_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Prepare the data\n",
    "df_draw = btc_df[['formatted_date', 'XRP-USD_volume', 'ETH-USD_volume', 'BTC-USD_volume','XMR-USD_volume'\n",
    ",'DAI-USD_volume','DOT-USD_volume']]\n",
    "df_draw['formatted_date'] = pd.to_datetime(df_draw['formatted_date'], format='%Y-%m-%d %I-%p')\n",
    "df_draw.set_index('formatted_date', inplace=True)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 6))  # Set the figure size to 12 inches wide and 6 inches high\n",
    "plt.plot(df_draw.index, df_draw['XRP-USD_volume'], label='Ripple')\n",
    "plt.plot(df_draw.index, df_draw['ETH-USD_volume'], label='ETH')\n",
    "plt.plot(df_draw.index, df_draw['BTC-USD_volume'], label='BTC')\n",
    "plt.plot(df_draw.index, df_draw['XMR-USD_volume'], label='Monero')\n",
    "plt.plot(df_draw.index, df_draw['DOT-USD_volume'], label='Polkadot')\n",
    "#plt.plot(df_draw.index, df_draw['DAI-USD_volume'], label='Dai')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Volume')\n",
    "plt.title('Cryptocurrency Volume Over Time')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_selected = btc_df.iloc[:, :7]\n",
    "btc_selected = btc_selected.round(0)\n",
    "btc_selected['formatted_date'] = btc_selected.index\n",
    "\n",
    "btc_selected.head()\n",
    "#---ETH-selecte------------\n",
    "eth_selected = btc_df.iloc[:, 7:13]\n",
    "eth_selected = eth_selected.round(0)\n",
    "eth_selected['formatted_date'] = eth_selected.index\n",
    "\n",
    "eth_selected\n",
    "#--BNB-selected------------\n",
    "bnb_selected = btc_df.iloc[:, 13:19]\n",
    "bnb_selected = bnb_selected.round(0)\n",
    "bnb_selected['formatted_date'] = bnb_selected.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate price changes\n",
    "btc_selected['price_changes'] = btc_selected['BTC-USD_close'].diff()\n",
    "btc_selected['price_changes'] = btc_selected['price_changes'].apply(lambda x: 'positive' if x > 0 else 'negative' if x < 0 else 'neutral')\n",
    "\n",
    "# Calculate price changes\n",
    "eth_selected['price_changes'] = eth_selected['ETH-USD_close'].diff()\n",
    "eth_selected['price_changes'] = eth_selected['price_changes'].apply(lambda x: 'positive' if x > 0 else 'negative' if x < 0 else 'neutral')\n",
    "eth_selected\n",
    "# Calculate price changes\n",
    "bnb_selected['price_changes'] = bnb_selected['BNB-USD_close'].diff()\n",
    "bnb_selected['price_changes'] = bnb_selected['price_changes'].apply(lambda x: 'positive' if x > 0 else 'negative' if x < 0 else 'neutral')\n",
    "bnb_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change columns name\n",
    "def remove_chars_before_underscore(df):\n",
    "    df.columns = df.columns.str.split('_').str[-1]\n",
    "remove_chars_before_underscore(btc_selected)\n",
    "remove_chars_before_underscore(eth_selected)\n",
    "remove_chars_before_underscore(bnb_selected)\n",
    "btc_selected\n",
    "eth_selected\n",
    "bnb_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add sentimeni type and compund to dataframe\n",
    "bnb_selected['date'] = pd.to_datetime(bnb_selected['date'])\n",
    "bnb_sentiment['created_at'] = pd.to_datetime(bnb_sentiment['created_at'])\n",
    "\n",
    "# Perform left merge on 'date' and 'created_at' columns\n",
    "bnb_selected = pd.merge(bnb_selected, bnb_sentiment[['created_at', 'compound', 'sentiment_type']],\n",
    "                     left_on='date', right_on='created_at', how='left')\n",
    "\n",
    "# Drop the redundant 'created_at' column\n",
    "bnb_selected = bnb_selected.drop('created_at', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# add sentimeni type and compund to dataframe\n",
    "eth_selected['date'] = pd.to_datetime(eth_selected['date'])\n",
    "eth_sentiment['created_at'] = pd.to_datetime(eth_sentiment['created_at'])\n",
    "\n",
    "# Perform left merge on 'date' and 'created_at' columns\n",
    "eth_selected = pd.merge(eth_selected, eth_sentiment[['created_at', 'compound', 'sentiment_type']],\n",
    "                     left_on='date', right_on='created_at', how='left')\n",
    "\n",
    "# Drop the redundant 'created_at' column\n",
    "eth_selected = eth_selected.drop('created_at', axis=1)\n",
    "\n",
    "eth_selected\n",
    "\n",
    "## add sentimeni type and compund to dataframe\n",
    "btc_selected['date'] = pd.to_datetime(btc_selected['date'])\n",
    "btc_sentiment['created_at'] = pd.to_datetime(btc_sentiment['created_at'])\n",
    "\n",
    "# Perform left merge on 'date' and 'created_at' columns\n",
    "btc_selected = pd.merge(btc_selected, btc_sentiment[['created_at', 'compound', 'sentiment_type']],\n",
    "                     left_on='date', right_on='created_at', how='left')\n",
    "\n",
    "# Drop the redundant 'created_at' column\n",
    "btc_selected = btc_selected.drop('created_at', axis=1)\n",
    "\n",
    "btc_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the top row\n",
    "bnb_selected = bnb_selected.iloc[1:]\n",
    "# Output the merged dataframe\n",
    "bnb_selected\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 20,
>>>>>>> 09b00b72d68b885fe2b0bc23c3ad96f7d5f3ea14
   "metadata": {},
   "outputs": [],
   "source": [
    "eth_selected.to_csv(r\"C:\\Users\\Lenovo\\Desktop\\Saturn project\\sentiment\\bnb_selected_with_sentiment_2023_01_02_2023_06_12.csv\")\n",
    "btc_selected.to_csv(r\"C:\\Users\\Lenovo\\Desktop\\Saturn project\\sentiment\\btc_selected_with_sentiment_2023_01_02_2023_06_12.csv\")\n",
    "bnb_selected.to_csv(r\"C:\\Users\\Lenovo\\Desktop\\Saturn project\\sentiment\\bnb_selected_with_sentiment_2023_01_02_2023_06_12.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD
   "source": [
    "print(eth_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
>>>>>>> 09b00b72d68b885fe2b0bc23c3ad96f7d5f3ea14
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.10.9"
=======
   "version": "3.11.1"
>>>>>>> 09b00b72d68b885fe2b0bc23c3ad96f7d5f3ea14
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
